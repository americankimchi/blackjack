{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0,1,2,3]\n",
    "b = a[:2]\n",
    "print(a)\n",
    "print(b)\n",
    "print(observation)\n",
    "print(observation[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_payout = 0 # to store total payout over 'num_rounds'\n",
    "average_payouts = []\n",
    "agent = Agent(env=env, epsilon=1.0, gamma=0.9, num_episodes_to_train=500, timestep=0)\n",
    "\n",
    "num_rounds = 800 # Payout calculated over num_rounds\n",
    "num_samples = 1 # num_rounds simulated over num_samples\n",
    "\n",
    "\n",
    "observation = env.reset()\n",
    "for sample in range(num_samples):\n",
    "    round = 1\n",
    "    epsilon_values = []\n",
    "    # Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0\n",
    "    while round <= num_rounds:\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "        action = agent.choose_action(observation)\n",
    "        next_observation, payout, is_done, _ = env.step(action)\n",
    "        agent.learn(observation, action, payout, next_observation)\n",
    "        total_payout += payout\n",
    "        observation = next_observation\n",
    "        if is_done:\n",
    "            agent.timestep += 1\n",
    "            observation = env.reset() # Environment deals new cards to player and dealer\n",
    "            round += 1\n",
    "            average_payouts.append(total_payout/(sample*num_rounds + round))\n",
    "\n",
    "# Plot epsilon over rounds to show rate of its decrease\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"num_rounds\")\n",
    "plt.ylabel(\"epsilon\")\n",
    "plt.plot(epsilon_values)\n",
    "plt.show()\n",
    "print (\"Average payout after {} rounds is {}\".format(num_rounds, total_payout/(num_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnedStrategy = open('AgentStrategy.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 1000 # Payout calculated over num_rounds\n",
    "num_samples = 100 # num_rounds simulated over num_samples\n",
    "\n",
    "payouts = []\n",
    "\n",
    "observation = env.reset()\n",
    "round = 1\n",
    "total_payout = 0 # to store total payout over 'num_rounds'\n",
    "# Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0\n",
    "while round <= num_rounds * num_samples:\n",
    "    action = agent.choose_action(observation)\n",
    "    next_observation, payout, is_done, _ = env.step(action)\n",
    "    agent.learn(observation, action, payout, next_observation)\n",
    "    payouts.append(payout)\n",
    "    observation = next_observation\n",
    "    if is_done:\n",
    "        observation = env.reset() # Environment deals new cards to player and dealer\n",
    "        round += 1\n",
    "\n",
    "num_observations = 0        \n",
    "list_players_hand = range(1, 22)\n",
    "list_dealers_upcard = range(1, 11)\n",
    "\n",
    "def readable_action(observation, agent):\n",
    "    \"\"\" \n",
    "    Pass observation to agent and get human readable action\n",
    "    H is hit, S is stick and '-' means the state is unseen and a random action is taken\n",
    "    \"\"\"\n",
    "    if observation not in agent.Q:\n",
    "        action = \"-\"\n",
    "    else:\n",
    "        strategy = agent.choose_action(observation)\n",
    "        if strategy == 2:\n",
    "            action = \"D\"\n",
    "        elif strategy == 1:\n",
    "            action = \"H\" \n",
    "        else:\n",
    "            action = \"S\"    \n",
    "    return action\n",
    "\n",
    "# Print headers to give more information about output\n",
    "learnedStrategy.write(\"{:^10} | {:^50} | {:^50}\".format(\n",
    "                      \"Player's\",\"Dealer's upcard when ace is not usable\",\n",
    "                      \"Dealer's upcard when ace is usable\"))\n",
    "learnedStrategy.write(\"{0:^10} | {1} | {1}\".format(\"Hand\", [str(upcard) if not upcard==10 else 'A' \n",
    "                                                        for upcard in list_dealers_upcard]))\n",
    "print (''.join(['-' for _ in range(116)]))\n",
    "for players_hand in list_players_hand:\n",
    "    actions_usable1, actions_usable2 = [], []\n",
    "    actions_not_usable1, actions_not_usable2 = [], []\n",
    "    for dealers_upcard in list_dealers_upcard:\n",
    "        observation = (players_hand, dealers_upcard, False, True)\n",
    "        actions_not_usable1.append(readable_action(observation, agent))\n",
    "        observation = (players_hand, dealers_upcard, False, False)\n",
    "        actions_not_usable2.append(readable_action(observation, agent))\n",
    "        \n",
    "        observation = (players_hand, dealers_upcard, True, True)\n",
    "        actions_usable1.append(readable_action(observation, agent))\n",
    "        observation = (players_hand, dealers_upcard, True, False)\n",
    "        actions_usable2.append(readable_action(observation, agent))\n",
    "    \n",
    "    learnedStrategy.write(\"{:>10} | {} | {}\".format(\n",
    "                          players_hand, actions_not_usable1, actions_usable1))\n",
    "    learnedStrategy.write(\"{:>10} | {} | {}\".format(\n",
    "                          players_hand, actions_not_usable2, actions_usable2))\n",
    "\n",
    "    \n",
    "learnedStrategy.write(\"Average payout after {} rounds is {}\".format(\n",
    "                       num_rounds, sum(payouts)/num_samples))\n",
    "\n",
    "learnedStrategy.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env=env, epsilon=1.0, gamma=0.1, num_episodes_to_train=15000, timestep=0)\n",
    "\n",
    "num_rounds = 1000 # Payout calculated over num_rounds\n",
    "num_samples = 1000 # num_rounds simulated over num_samples\n",
    "\n",
    "average_payouts = []\n",
    "\n",
    "observation = env.reset()\n",
    "for sample in range(num_samples):\n",
    "    round = 1\n",
    "    total_payout = 0 # to store total payout over 'num_rounds'\n",
    "    # Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0\n",
    "    while round <= num_rounds:\n",
    "        action = agent.choose_action(observation)\n",
    "        next_observation, payout, is_done, _ = env.step(action)\n",
    "        agent.learn(observation, action, payout, next_observation)\n",
    "        total_payout += payout\n",
    "        observation = next_observation\n",
    "        if is_done:\n",
    "            agent.timestep += 1\n",
    "            observation = env.reset() # Environment deals new cards to player and dealer\n",
    "            round += 1\n",
    "    average_payouts.append(total_payout)\n",
    "\n",
    "# Plot payout per 1000 episodes for each value of 'sample'\n",
    "plt.plot(average_payouts)           \n",
    "plt.xlabel('num_samples')\n",
    "plt.ylabel('payout after 1000 rounds')\n",
    "plt.show()      \n",
    "    \n",
    "print (\"Average payout after {} rounds is {}\".format(num_rounds, sum(average_payouts)/(num_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(a, b):\n",
    "    return float(a > b) - float(a < b)\n",
    "\n",
    "# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "deckVal = dict()\n",
    "for card in deck:\n",
    "    if card == 11 or card == 12 or card == 13:\n",
    "        deckVal[card] = 10\n",
    "    else:\n",
    "        deckVal[card] = card\n",
    "\n",
    "def draw_card(np_random):\n",
    "    return int(np_random.choice(deck))\n",
    "\n",
    "\n",
    "def draw_hand(np_random):\n",
    "    return [draw_card(np_random), draw_card(np_random)]\n",
    "\n",
    "def trueHandSum(hand):\n",
    "    trueSum = 0\n",
    "    for card in hand:\n",
    "        trueSum += deckVal[card]\n",
    "    return trueSum\n",
    "\n",
    "def isSplited(hand):\n",
    "    return type(hand[0]) == list\n",
    "\n",
    "def split(hand):\n",
    "    if isSplited(hand):\n",
    "        hand.append(hand[0][0])\n",
    "    else:\n",
    "        hand.append([hand[0]])\n",
    "    \n",
    "    return hand\n",
    "    \n",
    "def usable_ace(hand):  # Does this hand have a usable ace?\n",
    "    return 1 in hand and trueHandSum(hand) + 10 <= 21\n",
    "\n",
    "\n",
    "def sum_hand(hand):  # Return current hand total\n",
    "    if usable_ace(hand):\n",
    "        return trueHandSum(hand) + 10\n",
    "    return trueHandSum(hand)\n",
    "\n",
    "\n",
    "def is_bust(hand):  # Is this hand a bust?\n",
    "    return sum_hand(hand) > 21\n",
    "\n",
    "\n",
    "def score(hand):  # What is the score of this hand (0 if bust)\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "\n",
    "def is_natural(hand):  # Is this hand a natural blackjack?\n",
    "    return sorted(hand) == [1, 10]\n",
    "\n",
    "\n",
    "def is_firstRound(hand):\n",
    "    return len(hand) == 2\n",
    "\n",
    "def isPair(hand):\n",
    "    return hand[0] == hand[1]\n",
    "\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    \"\"\"Simple blackjack environment\n",
    "\n",
    "    Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    "    near as possible to 21 without going over.  They're playing against a fixed\n",
    "    dealer.\n",
    "    Face cards (Jack, Queen, King) have point value 10.\n",
    "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "    This game is placed with an infinite deck (or with replacement).\n",
    "    The game starts with each (player and dealer) having one face up and one\n",
    "    face down card.\n",
    "\n",
    "    The player can request additional cards (hit=1) until they decide to stop\n",
    "    (stick=0) or exceed 21 (bust).\n",
    "\n",
    "    After the player sticks, the dealer reveals their facedown card, and draws\n",
    "    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "\n",
    "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "    decided by whose sum is closer to 21.  The reward for winning is +1,\n",
    "    drawing is 0, and losing is -1.\n",
    "\n",
    "    The observation of a 3-tuple of: the players current sum,\n",
    "    the dealer's one showing card (1-10 where 1 is ace),\n",
    "    and whether or not the player holds a usable ace (0 or 1).\n",
    "\n",
    "    This environment corresponds to the version of the blackjack problem\n",
    "    described in Example 5.1 in Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto.\n",
    "    http://incompleteideas.net/book/the-book-2nd.html\n",
    "    \"\"\"\n",
    "    def __init__(self, natural=False):\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(32),\n",
    "            spaces.Discrete(11),\n",
    "            spaces.Discrete(2),\n",
    "            spaces.Discrete(2),\n",
    "            spaces.Discrete(2)))\n",
    "        self.seed()\n",
    "\n",
    "        # Flag to payout 1.5 on a \"natural\" blackjack win, like casino rules\n",
    "        # Ref: http://www.bicyclecards.com/how-to-play/blackjack/\n",
    "        self.natural = natural\n",
    "        # Start the first game\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if isSplited(self.player)\n",
    "        # split\n",
    "        if action == 3:\n",
    "            self.player = split(self.player)\n",
    "        # double\n",
    "        elif action == 2:\n",
    "            self.player.append(draw_card(self.np_random))\n",
    "            if is_bust(self.player):\n",
    "                done = True\n",
    "                reward = -2\n",
    "            else:\n",
    "                done = True\n",
    "                while sum_hand(self.dealer) < 17:\n",
    "                    self.dealer.append(draw_card(self.np_random))\n",
    "                reward = 2*cmp(score(self.player), score(self.dealer))\n",
    "        # hit: add a card to players hand and return\n",
    "        elif action == 1:\n",
    "            self.player.append(draw_card(self.np_random))\n",
    "            if is_bust(self.player):\n",
    "                done = True\n",
    "                reward = -1\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0\n",
    "        # stick: play out the dealers hand, and score\n",
    "        else:\n",
    "            done = True\n",
    "            while sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(draw_card(self.np_random))\n",
    "            reward = cmp(score(self.player), score(self.dealer))\n",
    "            if is_natural(self.player) and reward == 1:\n",
    "                reward = 1.5\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (sum_hand(self.player), self.dealer[0], usable_ace(self.player), is_firstRound(self.player), isPair(self.player))\n",
    "\n",
    "    def reset(self):\n",
    "        self.dealer = draw_hand(self.np_random)\n",
    "        self.player = draw_hand(self.np_random)\n",
    "        return self._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 2]\n",
      " [4 2]]\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "v = np.array([[1,2],[3,2],[4,2]])\n",
    "print(v)\n",
    "print(np.shape(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, epsilon=1.0, gamma=0.99, num_episodes_to_train=30000, timestep=0):\n",
    "        self.env = env\n",
    "\n",
    "        # Looks like n is number of valid actions from the souce code\n",
    "        self.valid_actions = list(range(self.env.action_space.n))\n",
    "\n",
    "        # Set parameters of the learning agent\n",
    "        self.Q = dict()          # Q-table which will be a dictionary of tuples\n",
    "        self.epsilon = epsilon   # Random exploration factor\n",
    "        #self.alpha = alpha       # Learning factor\n",
    "        self.gamma = gamma       # Discount factor- closer to 1 learns well into distant future\n",
    "        self.visits = dict()\n",
    "        \n",
    "        # epsilon will reduce linearly until it reaches 0 based on num_episodes_to_train\n",
    "        # epsilon drops to 90% of its inital value in the first 30% of num_episodes_to_train\n",
    "        # epsilon then drops to 10% of its initial value in the next 40% of num_episodes_to_train\n",
    "        # epsilon finally becomes 0 in the final 30% of num_episodes_to_train\n",
    "        self.num_episodes_to_train = num_episodes_to_train # Change epsilon each episode based on this\n",
    "        #self.small_decrement = (0.1 * epsilon) / (0.3 * num_episodes_to_train) # reduces epsilon slowly\n",
    "        #self.big_decrement = (0.8 * epsilon) / (0.4 * num_episodes_to_train) # reduces epilon faster\n",
    "\n",
    "        #self.num_episodes_to_train_left = num_episodes_to_train\n",
    "        self.timestep = timestep\n",
    "        \n",
    "    def updateEpsilon(self):\n",
    "        epsilonStart = 1\n",
    "        epsilonEnd = 0\n",
    "        scalingFactor = 100\n",
    "        self.epsilon = epsilonStart + (epsilonEnd-epsilonStart)*np.exp(\n",
    "                       (self.timestep-self.num_episodes_to_train)/scalingFactor)\n",
    "\n",
    "    def update_parameters(self):\n",
    "        \"\"\"\n",
    "        Update epsilon and alpha after each action\n",
    "        Set them to 0 if not learning\n",
    "        \"\"\"\n",
    "        '''\n",
    "        if self.num_episodes_to_train_left > 0.7 * self.num_episodes_to_train:\n",
    "            self.epsilon -= self.small_decrement\n",
    "        elif self.num_episodes_to_train_left > 0.3 * self.num_episodes_to_train:\n",
    "            self.epsilon -= self.big_decrement\n",
    "        elif self.num_episodes_to_train_left > 0:\n",
    "            self.epsilon -= self.small_decrement\n",
    "        else:\n",
    "            self.epsilon = 0.0\n",
    "            self.alpha = 0.0\n",
    "        '''\n",
    "        #self.timestep += 1\n",
    "        if self.timestep <= self.num_episodes_to_train:\n",
    "            self.updateEpsilon()\n",
    "        else:\n",
    "            self.epsilon = 0\n",
    "\n",
    "    def create_Q_if_new_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Set intial Q values to 0.0 if observation not already in Q table\n",
    "        \"\"\"\n",
    "        # If not first round, double and split is not allowed\n",
    "        if observation[3] == False:\n",
    "            self.valid_actions = self.valid_actions[:2]\n",
    "        else:\n",
    "            # If the player has a pair\n",
    "            if observation[4] == True:\n",
    "                self.valid_actions = list(range(self.env.action_space.n))\n",
    "            else:\n",
    "                self.valid_actions = self.valid_actions[:3]\n",
    "\n",
    "        if observation not in self.Q:\n",
    "            self.Q[observation] = dict((action, 0.0) for action in self.valid_actions)\n",
    "            self.visits[observation] = dict((action, 1) for action in self.valid_actions)\n",
    "\n",
    "    def get_maxQ(self, observation):\n",
    "        \"\"\"\n",
    "        Called when the agent is asked to find the maximum Q-value of\n",
    "        all actions based on the 'observation' the environment is in.\n",
    "        \"\"\"\n",
    "        self.create_Q_if_new_observation(observation)\n",
    "        return max(self.Q[observation].values())\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\"\n",
    "        Choose which action to take, based on the observation.\n",
    "        If observation is seen for the first time, initialize its Q values to 0.0\n",
    "        \"\"\"\n",
    "        \n",
    "        self.create_Q_if_new_observation(observation)\n",
    "\n",
    "        # uniformly distributed random number > epsilon happens with probability 1-epsilon\n",
    "        if random.random() > self.epsilon:\n",
    "            maxQ = self.get_maxQ(observation)\n",
    "\n",
    "            # multiple actions could have maxQ- pick one at random in that case\n",
    "            # this is also the case when the Q value for this observation were just set to 0.0\n",
    "            action = random.choice([k for k in self.Q[observation].keys()\n",
    "                                    if self.Q[observation][k] == maxQ])\n",
    "        else:\n",
    "            action = random.choice(self.valid_actions)\n",
    "        \n",
    "        self.visits[observation][action] += 1\n",
    "        self.update_parameters()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self, observation, action, reward, next_observation):\n",
    "        \"\"\"\n",
    "        Called after the agent completes an action and receives an award.\n",
    "        This function does not consider future rewards\n",
    "        when conducting learning.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Q = Q*(1-alpha) + alpha(reward + discount * utility of next observation)\n",
    "        # Q = Q - Q * alpha + alpha(reward + discount * self.get_maxQ(next_observation))\n",
    "        # Q = Q - alpha (-Q + reward + discount * self.get_maxQ(next_observation))\n",
    "        alpha = 1/(1+self.visits[observation][action])\n",
    "        self.Q[observation][action] = (1-alpha)*self.Q[observation][action] + (\n",
    "                                      (alpha)*(reward+(self.gamma*self.get_maxQ(next_observation))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
